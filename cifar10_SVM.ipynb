{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from __future__ import print_function\n",
    "from six.moves import cPickle as pickle\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.misc import imread\n",
    "import platform\n",
    "\n",
    "def load_pickle(f):\n",
    "    return  pickle.load(f)\n",
    "    raise ValueError(\"invalid python version: {}\".format(version))#check python version\n",
    "\n",
    "def load_CIFAR_batch(filename):\n",
    "    \"\"\" load single batch of cifar \"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        datadict = load_pickle(f)\n",
    "        X = datadict['data']\n",
    "        Y = datadict['labels']\n",
    "        X = X.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"float\")\n",
    "        Y = np.array(Y)\n",
    "        return X, Y #reference from Github\n",
    "\n",
    "def load_CIFAR10(ROOT):\n",
    "    \"\"\" load all of cifar \"\"\"\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for b in range(1,6):\n",
    "        f = os.path.join(ROOT, 'data_batch_%d' % (b, ))\n",
    "        X, Y = load_CIFAR_batch(f)\n",
    "        xs.append(X)\n",
    "        ys.append(Y)    \n",
    "    Xtr = np.concatenate(xs)\n",
    "    Ytr = np.concatenate(ys)\n",
    "    del X, Y\n",
    "    Xte, Yte = load_CIFAR_batch(os.path.join(ROOT, 'test_batch'))\n",
    "    return Xtr, Ytr, Xte, Yte#offical method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (50000, 32, 32, 3)\n",
      "Training labels shape:  (50000,)\n",
      "Test data shape:  (10000, 32, 32, 3)\n",
      "Test labels shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "cifar10_dir = 'cifar-10-batches-py'\n",
    "X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "\n",
    "print('Training data shape: ', X_train.shape)\n",
    "print('Training labels shape: ', y_train.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"samples_per_class = 5\\nfor y, cls in enumerate(classes):\\n    idxs = np.flatnonzero(y_train == y)\\n    idxs = np.random.choice(idxs, samples_per_class, replace=False)\\n    for i, idx in enumerate(idxs):\\n        plt_idx = i * num_classes + y + 1\\n        plt.subplot(samples_per_class, num_classes, plt_idx)\\n        plt.imshow(X_train[idx].astype('uint8'))\\n        plt.axis('off')\\n        if i == 0:\\n            plt.title(cls)\\nplt.show()\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "num_classes = len(classes)\n",
    "'''samples_per_class = 5\n",
    "for y, cls in enumerate(classes):\n",
    "    idxs = np.flatnonzero(y_train == y)\n",
    "    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
    "    for i, idx in enumerate(idxs):\n",
    "        plt_idx = i * num_classes + y + 1\n",
    "        plt.subplot(samples_per_class, num_classes, plt_idx)\n",
    "        plt.imshow(X_train[idx].astype('uint8'))\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title(cls)\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 49000 is out of bounds for axis 0 with size 49000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-74ca878dda82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# training set.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_training\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_training\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnum_validation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mX_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 49000 is out of bounds for axis 0 with size 49000"
     ]
    }
   ],
   "source": [
    "num_training = 49000\n",
    "num_validation = 1000\n",
    "num_test = 10000\n",
    "\n",
    "# Our validation set will be num_validation points from the original\n",
    "# training set.\n",
    "mask = range(num_training, num_training + num_validation)\n",
    "X_val = X_train[mask]\n",
    "y_val = y_train[mask]\n",
    "\n",
    "# Our training set will be the first num_train points from the original\n",
    "# training set.\n",
    "mask = range(num_training)\n",
    "X_train = X_train[mask]\n",
    "y_train = y_train[mask]\n",
    "\n",
    "\n",
    "# We use the first num_test points of the original test set as our\n",
    "# test set.\n",
    "mask = range(num_test)\n",
    "X_test = X_test[mask]\n",
    "y_test = y_test[mask]\n",
    "\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (49000, 3072)\n",
      "Validation data shape:  (1000, 3072)\n",
      "Test data shape:  (10000, 3072)\n",
      "dev data shape:  (500, 3072)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "\n",
    "#  print out the shapes of the data\n",
    "print('Training data shape: ', X_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_image = np.mean(X_train, axis=0)\n",
    "X_train -= mean_image\n",
    "X_val -= mean_image\n",
    "X_test -= mean_image\n",
    "X_dev -= mean_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 6000: loss 415.310053\n",
      "iteration 100 / 6000: loss 243.311367\n",
      "iteration 200 / 6000: loss 149.663177\n",
      "iteration 300 / 6000: loss 91.544937\n",
      "iteration 400 / 6000: loss 57.233706\n",
      "iteration 500 / 6000: loss 36.437141\n",
      "iteration 600 / 6000: loss 23.511397\n",
      "iteration 700 / 6000: loss 16.382672\n",
      "iteration 800 / 6000: loss 11.212185\n",
      "iteration 900 / 6000: loss 9.307514\n",
      "iteration 1000 / 6000: loss 7.146279\n",
      "iteration 1100 / 6000: loss 6.371967\n",
      "iteration 1200 / 6000: loss 6.014936\n",
      "iteration 1300 / 6000: loss 5.190503\n",
      "iteration 1400 / 6000: loss 5.294888\n",
      "iteration 1500 / 6000: loss 5.358771\n",
      "iteration 1600 / 6000: loss 5.438908\n",
      "iteration 1700 / 6000: loss 5.325443\n",
      "iteration 1800 / 6000: loss 5.189342\n",
      "iteration 1900 / 6000: loss 5.186030\n",
      "iteration 2000 / 6000: loss 5.347453\n",
      "iteration 2100 / 6000: loss 4.954253\n",
      "iteration 2200 / 6000: loss 5.053960\n",
      "iteration 2300 / 6000: loss 5.577104\n",
      "iteration 2400 / 6000: loss 5.418405\n",
      "iteration 2500 / 6000: loss 4.879032\n",
      "iteration 2600 / 6000: loss 4.906789\n",
      "iteration 2700 / 6000: loss 5.266607\n",
      "iteration 2800 / 6000: loss 5.380785\n",
      "iteration 2900 / 6000: loss 5.146866\n",
      "iteration 3000 / 6000: loss 5.168125\n",
      "iteration 3100 / 6000: loss 5.128840\n",
      "iteration 3200 / 6000: loss 5.133823\n",
      "iteration 3300 / 6000: loss 4.875658\n",
      "iteration 3400 / 6000: loss 4.398522\n",
      "iteration 3500 / 6000: loss 4.908901\n",
      "iteration 3600 / 6000: loss 4.682009\n",
      "iteration 3700 / 6000: loss 5.098026\n",
      "iteration 3800 / 6000: loss 5.099129\n",
      "iteration 3900 / 6000: loss 4.863309\n",
      "iteration 4000 / 6000: loss 5.499455\n",
      "iteration 4100 / 6000: loss 5.181822\n",
      "iteration 4200 / 6000: loss 5.026334\n",
      "iteration 4300 / 6000: loss 4.770645\n",
      "iteration 4400 / 6000: loss 4.790635\n",
      "iteration 4500 / 6000: loss 5.362300\n",
      "iteration 4600 / 6000: loss 4.966506\n",
      "iteration 4700 / 6000: loss 5.217393\n",
      "iteration 4800 / 6000: loss 4.550506\n",
      "iteration 4900 / 6000: loss 4.942037\n",
      "iteration 5000 / 6000: loss 5.025223\n",
      "iteration 5100 / 6000: loss 5.548079\n",
      "iteration 5200 / 6000: loss 5.076731\n",
      "iteration 5300 / 6000: loss 4.843164\n",
      "iteration 5400 / 6000: loss 5.141734\n",
      "iteration 5500 / 6000: loss 5.586905\n",
      "iteration 5600 / 6000: loss 5.060181\n",
      "iteration 5700 / 6000: loss 4.954513\n",
      "iteration 5800 / 6000: loss 4.808352\n",
      "iteration 5900 / 6000: loss 5.325924\n",
      "That took 16.766696s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class LinearSVM:\n",
    "    def __init__(self):\n",
    "        self.W = None\n",
    "    \n",
    "    def loss(self, X, y, reg):\n",
    "        \"\"\"\n",
    "        Structured SVM loss function, vectorized implementation.\n",
    "\n",
    "        Inputs and outputs are the same as svm_loss_naive.\n",
    "        \"\"\"\n",
    "        loss = 0.0\n",
    "        dW = np.zeros(self.W.shape) # initialize the gradient as zero\n",
    "\n",
    "        num_train = X.shape[0]\n",
    "        scores = X.dot(self.W)\n",
    "        correct_class_score = scores[range(num_train), list(y)].reshape(-1,1) # (N,1)\n",
    "        margin = np.maximum(0, scores - correct_class_score + 1)\n",
    "        margin[range(num_train), list(y)] = 0\n",
    "        loss = np.sum(margin) / num_train + 0.5 * reg * np.sum(self.W * self.W)\n",
    "  \n",
    "        num_classes = self.W.shape[1]\n",
    "        inter_mat = np.zeros((num_train, num_classes))\n",
    "        inter_mat[margin > 0] = 1\n",
    "        inter_mat[range(num_train), list(y)] = 0\n",
    "        inter_mat[range(num_train), list(y)] = -np.sum(inter_mat, axis=1)\n",
    "\n",
    "        dW = (X.T).dot(inter_mat)\n",
    "        dW = dW/num_train + reg*self.W\n",
    "\n",
    "        return loss, dW\n",
    "    \n",
    "    def train(self, X, y, learning_rate=1e-3, reg=1e-5, num_iters=100,\n",
    "            batch_size=200, verbose=False):\n",
    "        num_train, dim = X.shape\n",
    "        num_classes = np.max(y) + 1 # assume y takes values 0...K-1 where K is number of classes\n",
    "        if self.W is None:\n",
    "            # lazily initialize W\n",
    "            self.W = 0.001 * np.random.randn(dim, num_classes)\n",
    "\n",
    "        # Run stochastic gradient descent to optimize W\n",
    "        loss_history = []\n",
    "        for it in range(num_iters):\n",
    "            X_batch = None\n",
    "            y_batch = None\n",
    "            idx_batch = np.random.choice(num_train, batch_size, replace = True)\n",
    "            X_batch = X[idx_batch]\n",
    "            y_batch = y[idx_batch]\n",
    "\n",
    "            # evaluate loss and gradient\n",
    "            loss, grad = self.loss(X_batch, y_batch, reg)\n",
    "            loss_history.append(loss)\n",
    "\n",
    "            self.W -=  learning_rate * grad\n",
    "\n",
    "            if verbose and it % 100 == 0:\n",
    "                print('iteration %d / %d: loss %f' % (it, num_iters, loss))\n",
    "\n",
    "        return loss_history\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = np.zeros(X.shape[0])\n",
    "        scores = X.dot(self.W)\n",
    "        y_pred = np.argmax(scores, axis = 1)\n",
    "        return y_pred\n",
    "\n",
    "import time\n",
    "svmlinear = LinearSVM()\n",
    "tic = time.time()\n",
    "loss_hist = svmlinear.train(X_train, y_train, learning_rate=1e-7, reg=2.5e4,\n",
    "                      num_iters=6000, verbose=True)\n",
    "toc = time.time()\n",
    "print('That took %fs' % (toc - tic))#running time\n",
    "plt.plot(loss_hist)\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('Loss value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 0.378980\n",
      "validation accuracy: 0.386000\n",
      "(49000,)\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = svmlinear.predict(X_train)\n",
    "print('training accuracy: %f' % (np.mean(y_train == y_train_pred), ))\n",
    "y_val_pred = svmlinear.predict(X_val)\n",
    "print('validation accuracy: %f' % (np.mean(y_val == y_val_pred), ))\n",
    "print (y_train_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Must provide either V or VI for Mahalanobis distance",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-c575757bccb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                            \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mahalanobis'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                            )\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0my_val_pred_KNN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'validation accuracy: %f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_val_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zhiyuzhang/anaconda2/lib/python2.7/site-packages/sklearn/neighbors/base.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    914\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 916\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zhiyuzhang/anaconda2/lib/python2.7/site-packages/sklearn/neighbors/base.pyc\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    252\u001b[0m             self._tree = BallTree(X, self.leaf_size,\n\u001b[1;32m    253\u001b[0m                                   \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meffective_metric_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m                                   **self.effective_metric_params_)\n\u001b[0m\u001b[1;32m    255\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_method\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'kd_tree'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m             self._tree = KDTree(X, self.leaf_size,\n",
      "\u001b[0;32msklearn/neighbors/binary_tree.pxi\u001b[0m in \u001b[0;36msklearn.neighbors.ball_tree.BinaryTree.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msklearn/neighbors/dist_metrics.pyx\u001b[0m in \u001b[0;36msklearn.neighbors.dist_metrics.DistanceMetric.get_metric\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msklearn/neighbors/dist_metrics.pyx\u001b[0m in \u001b[0;36msklearn.neighbors.dist_metrics.MahalanobisDistance.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Must provide either V or VI for Mahalanobis distance"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf = KNeighborsClassifier(n_neighbors=16,\n",
    "                           weights='distance',     #{distance, uniform}\n",
    "                           algorithm = 'auto',\n",
    "                           metric='mahalanobis'\n",
    "                           )\n",
    "clf = clf.fit(X_train, y_train)\n",
    "y_val_pred_KNN = clf.predict(X_val)\n",
    "print('validation accuracy: %f' % (np.mean(y_val == y_val_pred), ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
